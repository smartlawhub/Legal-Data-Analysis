# Clustering

<b>1. </b>Given a dataset, how can you partition it in groups that relate to each other ?

There exists a set of tools that do just this, based on algorithms devised by mathematicians. One of the most 
popular is called k-means: given a number <i>k</i> and a set of distances between these points, the algorithm is 
seeking a way to divide the data  into <i>k</i> groups, where each point is from the closest central point possible. 
This process is repeated until the clusters are stable. Algorithms work best when the data is normalized somewhat, to reduce the variations 
created by outliers.

For a k-means algorithm, you try to find a number k that <i>prima facie</i> would make sense, or repeat the analysis 
until you find one. Other methods (HDBSCAN) are more agnostic, in that they will identify a number of possible 
groups by themselves, but might leave some data points out of any group because they don't fit (k-means try to 
include all data points).

<b>2. </b>These methods work well for mathematical data (it's easier to calculate the distance between two points at 
position y and x), but further steps are needed for text data. 

You will need notably to reduce the text to mathematical points. A popular way to do so is by calculating the TF-IDF 
score of each word in the data. TF-IDS stands for "term frequencyâ€“inverse document frequency", and is a measure of 
how important a word is in a dataset: you first measure how frequent it is, and compare with how frequent you would 
expect it to be (for instance, "the" is frequent, but this is not surprising given how frequent it is in any dataset). This results in a matrix with measures of the importance of all terms for a given text compared to the dataset. 

(Check out, for instance, <a href="https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html">this neat analysis</a> of US Presidential Inauguration speeches using Tf-IDF.)

<b>3. </b>Let's try to apply this to a dataset of clauses, this time from investment treaties. You'll find in file 
such a dataset, which I collected from around 1,600 bilateral investment treaties. Most of the text is boilerplate: 
it gets repeated over and over. However, the order of the articles frequently changes. Given this, we'll use an 
k-means algorithm to classify each clause into a group.

We first need to create the dataset from the .csv I provided you with.

Then once we have a dataset of the format "one line:one cleaned data point", we are ready to transform it into a 
vector using the tf-idf algorithm. There are a few parameters you can play around, such as the size of n-grams 
(groups of words), or the max number of features: higher can be more accurate, but also less efficient, so there is 
a lot of trial and errors involved in these endeavours.

The standard approach is first to initialise an object based on the selected methods with selected parameters (our 
`tfidf` object here), and then to use the `.fit_transform` function to create a new object with our data. We are 
doing it twice here: once to vectorise based on the 250 most relevant words; and a second time to reduce these 250 dimensions to 10.

Once we have some data that is workable (i.e., all our texts over 10 dimensions), we can finally use the algorithm - 
with the same method (except here we use the fit_predict method to immediately get the groupings).

We can then (i) check that the labels make sense in an excel spreadsheet; and (ii) display these dimensions in a 2d 
scatter plot to also get a sense of what's going on.

Note that there are better ways to do this: here the scatter plot is not super great, so it's good at indicating the 
clauses that are close to one another (see the dispute settlement ones !).
