# Prediction

<b>1. </b> Machine learning prediction is not that hard - at least if you want to get quick results. Customisation 
(to your type of data) is what takes time, which sometimes is not worth it. (Sometimes that time would be better 
spent acquiring more data, as this is a chief input for most machine learning applications: better data > better model.)

In general, all applications of machine learning aim at finding a function that best maps an input X to an output y.

<b>2. </b>We'll perform a basic machine learning regression over Cour de cassation judgments, and see if, given a 
moyen, we are able to predict the result (cassation or rejet).

After executing all the associated code, you'll see that yes: our (very basic, even dumb) application of machine 
learning works in predicting cassation judgments. But now is time for a few caveats, such as:

<ul><li><b>Size</b> How big your dataset is is super important, and 800-odd judgments is not great in that regard;</li>
<li><b>Distribution</b> In front of <i>any</i> phenomenon, this should be your first question: "what is the 
distribution". You might discover that surprising things are not so surprising put in context; here, it might be 
relevant to our problem that our dataset was already skewed with 64% cassation compared to rejets;</li>
<li><b>Model</b> Different models might lead to different results, sometimes better, sometimes not, and you should 
make sure to try a few; and</li><li><b>Generalisation</b> Depending on the method you use, your model could be very 
good at predicting on your training dataset, but terrible at new data, which is called overfitting. That happens, 
for instance, when your model is incapable of ignoring noise or outliers.</li></ul>

There are ways to deal with all these aspects. Size would require you to acquire more data (and computing power), 
which is not that hard. Distribution can be fixed by using methods to balance a dataset (over- or under-sampling, 
adding weights or thresholds). Different models should be tried and compared, and different types of models 
(parametric or linear) have different ways of dealing with overfitting.

<b>3. </b>This is why it's important is to investigate what makes a given model perform better.

A first step in this respect is to calculate basic statistical metrics, such as:
<ul>
<li><b>Precision</b> How accurate are your predictions, or how valid are your results, or whether you have a lot 
of false positives;</li>
<li><b>Recall</b> How good are you at retrieving signal in your dataset, or how complete are your results, or whether you 
have a lot of false negatives;</li>
<li><b>F1 Score</b> For binary systems, is a measure of accuracy over your dataset, based on precision and recall; 
closer to 1 is better;</li>
<li><b>Cumulative Gain </b> How does your model performs as opposed to chance.  
</li></ul>

A second step is to evaluate your model over several subsamples of data, with methods such as k-fold (splitting your 
dataset in k non-overlapping subsamples) and bootstrapping (creating multiple subsamples from the dataset). The idea 
is that, if your model performs well, scores should be relatively stable over the subsamples.

Thirdly, you can investigate your model; one way to do this is to identify the features/vectors that have the most 
weight according to the classifier. A module called <code>eli5</code> (for "Explain Like I'm 5") helps in this 
respect, outputting the most relevant (i.e., "weighty") features.

<b>4. </b>Finally, we can try other models and compare their accuracy. Here, I try three distinct methods, all 
available from the sklearn package.

<ul>
<li><b>Logistic Regression</b> What we did earlier - this is related to the traditional "linear regression", or the 
least-square methods of finding the line that is the closest to the most data points in a scatterplot, though 
"logit" is (usually) made for classification purposes. There are plenty of variations, however (the 
<code>solver</code> argument in sklearn's classifier), and 
it is not always 
fit for purposes;
</li>
<li><b>Trees and Forests</b> On the assumption that features are nodes in a decision tree that links such features 
to outputs (our labels), this family of algorithms will randomly tries multiple combinations of features->output 
until it finds one or a number of trees (the "Forest") that best predicts new data. The <code>max-depth</code> 
argument helps preventing overfitting. A variant of this is the Gradiant Boosting Tree, that goes from tree to tree 
trying to explain the errors of the preceding tree;</li>
<li><b>Linear SVC</b> Or "support vector classifier",an algorithm that seeks the largest "gap" in a map with all the 
features (given existing labels), so as to assign any data point to one subpart or the map. SVM models are related;</li>
<li><b>Naive Bayes </b> Bayes Theorem is a statistical approach to probability, that links existing probs to 
observations in a continuous loop, or, in other words, the probability that A happens knowing that B happened (as 
put by a popular blogger: <blockquote>P(A|B) = [P(A)*P(B|A)]/P(B), all the rest is commentary.</blockquote>.  <a 
href="https://xkcd.com/1132/">Here is a good way to see the difference with the main approach to probabilities</a>. 
Naive Bayes approach a dataset with the idea that the features are in fact independent from each others and 
contribute equally to the prediction (not true, some words are more important, but helps computationally and returns 
good results in general).
Here as well, different flavors of NB, with the GaussianNB assuming a normal distribution of your dataset,
</li></ul>
