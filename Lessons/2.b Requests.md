## Requests

<b>1. </b>We'll start by introducing a basic scraping (one 'p') tool, the `requests` module, and we'll start by 
scraping a basic website, such as HEC's master presentation webpage (<a href="https://www.hec.edu/fr/grande-ecole-masters/ms-et-msc/ms/llm-droit-et-management-international/programme">here</a>).

*** A few words about how the internet works (good overview <a href="https://www.explainthatstuff.com/internet.html">here</a>) ***

<b>2. </b>We can describe this type of webpage as "static", because every one sending a request to the server to 
this page will see the same result, and you won't be able to change it. It's irrelevant that you can fold/unfold 
elements, because it's just a trick: the data is already there, it's not being fetched as you click "unfold" (if you 
check the page source, you'll see all the data).

By contrast, dynamic websites are much more complex, and for scraping purposes the main issue is that they fetch 
data that is not available or visible in the html code, or not immediately. If you check on the page source, you'll 
typically see the framework in which the page's content is located, but not the content itself. You can also think 
of it as to how the data is processed: fully by the client (i.e., you) for static webpages, and in part by the server for dynamic pages.
We'll come back on dynamic websites in the next task, about Selenium. 

<b>2. </b> The distinction between static and dynamic is often blurry, as we'll see, but to the extent possible it's easier to 
work with static websites, so I'd advise to seek short cuts. For instance, querying a database with a search term 
can be dynamic (the webpage will depend on the result), but you can make it static in some circumstances, by 
including the request in the URL, and fiddling with it (example <a href="https://recherche.conseil-constitutionnel.fr/?expert=2&q=Libert%C3%A9">here</a>: see how you can manipulate the URL to get the required results, which are 
then static, or mostly so).

<b>3. </b> Now, suppose you want to scrap the French constitution from the website of the Conseil constitutionnel, 
it's pretty easy: you just ask `requests` to fetch the page, and store it in a variable.

Yet, as you can see, a few more steps are needed to get the Constitution, as opposed to the page storing the 
constitution, and possibly without the `html` code surrounding it.

(Note that, quite often, you'll want to scrap several distinct pages of a website, something you can do with a loop. 
Sometimes, you don't know in advance what are the pages you'll want to scrap, and there are third-party plugins, 
known as crawlers, for this.)

<b>4. </b>To go past this issue, you'll need to manipulate the  `html` and locate what you need. To do so, a module 
called BeautifulSoup is very helpful. But also your browser: what you want to do is:

<ul><li>Go to the webpage on your Browser</li>
    <li>Inspect or try to find the page source (on Chrome, that would be Right-Click, `Inspect`)</li>
    <li>Locate the highest element holding the data you are looking for</li>
    <li>Find a way to locate that element with BeautifulSoup</li>
<li>Perform operations over that element (for instance, extract text)</li>
    </ul>

<b>5. </b>This is the easiest part; often, however, you only need one part of a webpage, or want to add conditions 
to the BeautifulSoup search criteria. One way to do this is to remember that `html`, like `xml`, is a structured 
language: elements have children and parents, over which you can iterate. For instance, the following will print 
every title (article numbers, here), in the Constitution.

To get to the text of the article, once again you need to study the structure of the webpage. Here you'll see that 
each title has siblings, which are `<p>` elements, and which enclose the relevant text. Here is some code to 
reconstruct the Constitution in a dictionary.

<u>Exercise 9</u> The Constitution is divided in parts and sub-sections. Can you write an algorithm that returns the 
section with the most articles (subarticles of the form "56-X" count as one). There are several ways to go about it.

<div class="hint">To locate elements by text, you can use the command of the form:
<code>element.find(Target Name, text=re.compile(your regex pattern here))</code>`
</div>
