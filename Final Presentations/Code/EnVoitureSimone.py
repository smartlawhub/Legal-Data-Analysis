Importation des librairies utiles
In [ ]:
from selenium import webdriver
from selenium.webdriver.common.by import By
import concurrent.futures as cf
import pandas as pd
import json
import matplotlib.pyplot as plt
import requests
import re
N'ayant pas besoin d'int√©ragir avec les √©l√©ments de la page, le module requests aurait √©t√© suffisant et m√™me plus efficace. Cependant le site du gouvernement est prot√©g√© par Incapsula contre le scraping, il est donc n√©cessaire d'utiliser Selenium qui utilise un vrai navigateur (mais qui est donc plus lent).
In [ ]:
def getDriver():
    options = webdriver.ChromeOptions()
    options.binary_location = "C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe"
    options.add_argument("--headless")
    return webdriver.Chrome(chrome_options=options, executable_path="C:\\Windows\\chromedriver.exe")
In [ ]:
base_url = "https://www.legifrance.gouv.fr/search/juri?tab_selection=juri&searchField=ALL&query=*&searchType=ALL&dateDecision=01%2F01%2F1946+%3E+01%2F01%2F2023&cassPubliBulletin=T&cassPubliBulletin=F&cassDecision=ARRET&cassFormation=ASSEMBLEE_PLENIERE&cassFormation=CHAMBRE_MIXTE&cassFormation=CHAMBRES_REUNIES&cassFormation=CHAMBRE_CIVILE_1&cassFormation=CHAMBRE_CIVILE_2&cassFormation=CHAMBRE_CIVILE_3&cassFormation=CHAMBRE_COMMERCIALE&cassFormation=CHAMBRE_SOCIALE&cassFormation=CHAMBRE_CRIMINELLE&cassFormation=COMMISSION_REEXAMEN&cassFormation=COMMISSION_REPARATION_DETENTION&cassFormation=COMMISSION_REVISION&cassFormation=COUR_REVISION&cassFormation=JURIDICTION_NATIONALE_LIBERTE_CONDITIONNELLE&cassFormation=ORDONNANCE_PREMIER_PRESIDENT&cassDecisionAttaquee=COMMISSION_INDEMNISATION_VICTIMES_INFRACTIONS&cassDecisionAttaquee=CONSEIL_PRUDHOMME&cassDecisionAttaquee=COUR_APPEL&cassDecisionAttaquee=COUR_ASSISES&cassDecisionAttaquee=COUR_CASSATION&cassDecisionAttaquee=COUR_JUSTICE_REPUBLIQUE&cassDecisionAttaquee=COUR_NATIONAL_INCAPACITE_TARIFICATION&cassDecisionAttaquee=TRIBUNAL_CORRECTIONNEL&cassDecisionAttaquee=TRIBUNAL_COMMERCE&cassDecisionAttaquee=TRIBUNAL_GRANDE_INSTANCE&cassDecisionAttaquee=TRIBUNAL_POLICE&cassDecisionAttaquee=TRIBUNAL_PREMIERE_INSTANCE&cassDecisionAttaquee=TRIBUNAL_AFFAIRES_SECURITE_SOCIALE&cassDecisionAttaquee=TRIBUNAL_FORCES_ARMEES&cassDecisionAttaquee=TRIBUNAL_INSTANCE&cassDecisionAttaquee=TRIBUNAL_CONTENTIEUX_INCAPACITE&cassDecisionAttaquee=TRIBUNAL_MARITIME_COMMERCIAL&cassDecisionAttaquee=TRIBUNAL_PARITAIRE_BAUX_RURAUX&cassDecisionAttaquee=TRIBUNAL_SUPERIEURS_APPEL&juridictionJudiciaire=Cour+de+cassation&typePagination=DEFAULT&sortValue=DATE_ASC&pageSize=100&page=1&tab_selection=juri#juri"
driver = getDriver()
driver.get(base_url)
Ce code permet de retrouver le nombre de pages qu'il va falloir parcourir :
nombre de pages
In [ ]:
nb_pages = int(
    driver.find_element(By.CLASS_NAME, "container-pager")\
          .find_elements(By.CLASS_NAME, "pager-item")[3]\
          .find_element(By.TAG_NAME, "span").get_attribute("innerHTML")
)
Le but dans un premier temps est de parcourir ces 4691 pages de r√©sultats pour r√©cup√©rer les 100 liens par page
Fonction permettant de trouver l'ann√©e dans une phrase
In [ ]:
def extractYear(sentence):
    ys = [str(y) for y in range(1946, 2024)]
    for y in ys:
        if (sentence.find(y) > 0):
            return int(y)
Liste des pages √† parcourir
In [ ]:
pages = [f"https://www.legifrance.gouv.fr/search/juri?tab_selection=juri&searchField=ALL&query=*&searchType=ALL&dateDecision=01%2F01%2F1946+%3E+01%2F01%2F2023&cassPubliBulletin=T&cassPubliBulletin=F&cassDecision=ARRET&cassFormation=ASSEMBLEE_PLENIERE&cassFormation=CHAMBRE_MIXTE&cassFormation=CHAMBRES_REUNIES&cassFormation=CHAMBRE_CIVILE_1&cassFormation=CHAMBRE_CIVILE_2&cassFormation=CHAMBRE_CIVILE_3&cassFormation=CHAMBRE_COMMERCIALE&cassFormation=CHAMBRE_SOCIALE&cassFormation=CHAMBRE_CRIMINELLE&cassFormation=COMMISSION_REEXAMEN&cassFormation=COMMISSION_REPARATION_DETENTION&cassFormation=COMMISSION_REVISION&cassFormation=COUR_REVISION&cassFormation=JURIDICTION_NATIONALE_LIBERTE_CONDITIONNELLE&cassFormation=ORDONNANCE_PREMIER_PRESIDENT&cassDecisionAttaquee=COMMISSION_INDEMNISATION_VICTIMES_INFRACTIONS&cassDecisionAttaquee=CONSEIL_PRUDHOMME&cassDecisionAttaquee=COUR_APPEL&cassDecisionAttaquee=COUR_ASSISES&cassDecisionAttaquee=COUR_CASSATION&cassDecisionAttaquee=COUR_JUSTICE_REPUBLIQUE&cassDecisionAttaquee=COUR_NATIONAL_INCAPACITE_TARIFICATION&cassDecisionAttaquee=TRIBUNAL_CORRECTIONNEL&cassDecisionAttaquee=TRIBUNAL_COMMERCE&cassDecisionAttaquee=TRIBUNAL_GRANDE_INSTANCE&cassDecisionAttaquee=TRIBUNAL_POLICE&cassDecisionAttaquee=TRIBUNAL_PREMIERE_INSTANCE&cassDecisionAttaquee=TRIBUNAL_AFFAIRES_SECURITE_SOCIALE&cassDecisionAttaquee=TRIBUNAL_FORCES_ARMEES&cassDecisionAttaquee=TRIBUNAL_INSTANCE&cassDecisionAttaquee=TRIBUNAL_CONTENTIEUX_INCAPACITE&cassDecisionAttaquee=TRIBUNAL_MARITIME_COMMERCIAL&cassDecisionAttaquee=TRIBUNAL_PARITAIRE_BAUX_RURAUX&cassDecisionAttaquee=TRIBUNAL_SUPERIEURS_APPEL&juridictionJudiciaire=Cour+de+cassation&typePagination=DEFAULT&sortValue=DATE_ASC&pageSize=100&page={i}&tab_selection=juri#juri" for i in range(1, nb_pages+1)]
Fonction qui scrap les informations utiles (lien et ann√©e), page par page
In [ ]:
def retrieve(url):
    to_add = []
    dr = getDriver()
    dr.get(url)
    articles = dr.find_elements(By.CLASS_NAME, "result-item")
    for article in articles:
        a = article.find_element(By.TAG_NAME, 'a')
        link = a.get_attribute("href")
        title = a.get_attribute("innerText")
        to_add.append((link, extractYear(title)))
    dr.quit()
    return to_add
Code qui r√©cup√©re r√©element les liens et ann√©es. 10 navigateurs sont lanc√©s en paral√®lle pour acc√©l√©rer le processus. Les r√©sultats sont finalement enregistr√©s dans un fichier json
In [ ]:
with cf.ThreadPoolExecutor(max_workers=10) as executor:
   futures = [executor.submit(retrieve, url) for url in pages]
   for future in cf.as_completed(futures):
      try:
         results = [future.result() for future in futures]
         with open("data.json", "w") as json_file:
            json.dump(results, json_file)
      except requests.ConnectTimeout:
         pass
Malheuresement, le site du gouvernement nous complique une fois de plus la t√¢che en nous autorisant l'acc√®s aux 10 000 premiers r√©sultats seulement.
affichage limite
Nos datas s'arr√™tent donc √† 1973. Pour palier √† √ßa, nous pourrions affiner la fen√™tre temporelle de la recherche ann√©e par ann√©e et r√©p√©ter le processus ci-dessus
fenetre temporelle
Formatage des datas dans un DataFrame pandas
In [ ]:
with open("data.json", "r") as json_file:
    data = json.load(json_file)
data = [sub for sub in data if len(sub) > 0]
flat = []
for sub in data:
    for item in sub:
        flat.append(item)
df = pd.DataFrame(flat, columns=['url', 'year'])
On se limite √† maximum 100 donn√©es par an
In [ ]:
to_concat = []
for y in set(df['year']):
    subdf = df[df['year'] == y]
    to_concat.append(subdf.sample(n = min(100, len(subdf)), random_state = 42))
normalized = pd.concat(to_concat, axis=0).reset_index(drop=True)
On va maintenant scraper les 688 liens des jugements que nous avons gard√©s pour en retirer les informations qui nous int√©ressent:
Date exacte
Sexe du pr√©sident
Juridiction
In [ ]:
results = {'date': [], 'gender': [], 'juridiction': []}
Le multithreading aurait pu √™tre utilis√© ici aussi, mais le nombre d'it√©rations est suffisament faible pour s'en affranchir
In [ ]:
driver = getDriver()
patternF = r"Mme "
patternM = r"M\.|M "
patternDate = r"\b\d{1,2}\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}\b"
for url in normalized['url']:
    driver.get(url)
    try:
        juridiction = driver.find_elements(By.CLASS_NAME, "horsAbstract")[0]\
                            .get_attribute("innerText")
        juridiction = juridiction[juridiction.find("- ")+2:]
    except Exception as e:
        juridiction = ''
        print("An error occurred:", str(e))
    try:
        pdt = driver.find_elements(By.CLASS_NAME, 'frame-block')[1]\
              .find_elements(By.TAG_NAME,'div')[0]\
              .find_elements(By.TAG_NAME, 'dd')[0]\
              .get_attribute("innerText")
        if re.findall(patternF, pdt, re.IGNORECASE):
            gender = 'F'
        elif re.findall(patternM, pdt, re.IGNORECASE):
            gender = 'M'
        else:
            gender = ''
    except Exception as e:
        gender = ''
        print("An error occurred:", str(e))
    try:
        date = driver.find_elements(By.CLASS_NAME, "horsAbstract")[1]\
                            .get_attribute("innerText")
        date = re.findall(patternDate, date, re.IGNORECASE)[0]
    except Exception as e:
        date = ''
        print("An error occurred:", str(e))
    results['date'].append(date)
    results['gender'].append(gender)
    results['juridiction'].append(juridiction)
Les r√©sultats sont mis en forme dans un DataFrame puis sont sauv√©s dans un fichier csv
In [ ]:
results = pd.DataFrame(results)
results.to_csv("dataframe.csv", index=False)
Il n'y aurait qu'une femme qui a pr√©sid√© entre 1946 et 1973 üòÖ
In [48]:
results[results['gender'] == 'F']
Out[48]:
date	gender	juridiction
58	28 octobre 1957	F	Chambre sociale
On se rend compte qu'il y a beacoup de donn√©es manquantes
In [49]:
print(f"Nombre de jugements √©tudi√©s: {len(results)}\nNombre de jugements o√π le sexe du pr√©sident est indiqu√©: {len(results[results['gender'] != ''])}")
Nombre de jugements √©tudi√©s: 689
Nombre de jugements o√π le sexe du pr√©sident est indiqu√©: 571
On retire en suite les lignes vides
In [ ]:
clean = results[results['gender'] != '']
Puis on convertit les dates en datetime, de sorte √† avoir des time series
In [ ]:
mois = {" janvier ": "/01/", " f√©vrier ": "/02/", " mars ": "/03/", " avril ": "/04/", " mai ": "/05/", " juin ": "/06/", " juillet ": "/07/", " ao√ªt ": "/08/", " septembre ": "/09/", " octobre ": "/10/", " novembre ": "/11/", " d√©cembre ": "/12/"}
def convert(dt):
    for m in mois:
        if m in dt:
            return dt.replace(m, mois[m])
In [ ]:
clean.loc[:, 'date'] = clean['date'].apply(convert)
clean.loc[:, 'date'] = pd.to_datetime(clean['date'], format='%d/%m/%Y')
clean.set_index('date', inplace=True)
Enfin, on compte le nombre de femmes pr√©sidentes par ann√©e et on affiche le graphe des r√©sultats
In [ ]:
serie = clean.groupby(clean.index.year)['gender'].sum()
serie = serie.apply(lambda val: val.count('F'))
In [50]:
serie.plot.bar()
plt.xlabel('Year')
plt.ylabel('# Women Presidents')
plt.title('Women presidents by year')
plt.show()
