{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fswiss\fcharset0 Helvetica-BoldOblique;
\f3\fswiss\fcharset0 Helvetica-Oblique;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red0\green0\blue0;
}
{\*\expandedcolortbl;;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985\c0;\csgray\c0;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww19840\viewh13180\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
	
\f1\b \ul Premi\'e8re \'e9tape - SCRAPPING : scrapper le CGI 
\f0\b0 \ulnone \
\
In [?]: for code in CGI:\
   ...:     driver.get("https://www.legifrance.gouv.fr/" + codes[code])\
   ...:     soup = BeautifulSoup(driver.page_source)\
   ...:     set_articles = soup.find_all("a", string=re.compile("^Article"))\
   ...:     for article in set_articles:\
   ...:         legiarti_match = re.search("LEGIARTI.*?#", article.get("href"))\
   ...:         if legiarti_match:\
   ...:             legiarti = legiarti_match.group()[:-1]\
   ...:         else:\
   ...:             continue\
   ...:         driver.get("https://www.legifrance.gouv.fr/codes/article_lc/" + legiarti)\
   ...:         time.sleep(1)\
   ...:         driver.find_element(By.XPATH, ".//button[@data-articleid='" + legiarti + "']|.//bu\
   ...: tton[@data-articlecid='" + legiarti + "']").click()\
   ...:         time.sleep(2)\
   ...:         article_num = driver.find_element(By.CLASS_NAME, "name-article").text\
   ...:         version_urls = [x.get_attribute("href") for x in driver.find_elements(By.CLASS_NAM\
   ...: E, "version")]\
   ...:         count_version = len(version_urls)\
   ...:         for version_url in [x for x in version_urls if x is not None]:\
   ...:             driver.get(version_url)\
   ...:             legiver = re.search("LEGIARTI.*?/", version_url).group()[:-1]\
   ...:             time.sleep(1)\
   ...:             driver.find_element(By.XPATH, ".//button[@data-articleid='" + legiver + "']|//\
   ...: button[@data-num='" + article_num.split(" ")[-1] + "']").click()\
   ...:             time.sleep(1)\
   ...:             try:\
   ...:                 origin = driver.find_element(By.CLASS_NAME, "date").text\
   ...:             except:\
   ...:                 origin = ""\
   ...:             article_text = driver.find_elements(By.CLASS_NAME, "content")[2].text\
   ...:             temp_list = [article_num, version_url.split("/")[-2], count_version, article_t\
   ...: ext, origin]\
   ...:             count_version -= 1\
   ...:             data_scrapped.append(temp_list)\
   ...: driver.close()\
   ...: \
\
\
	
\f1\b 1.2. En cas d'erreur lors du scrapping, une fonction permet de modifier la variable set_articles pour faire reprendre le scrapping \'e0 un endroit voulu. Exemple : 
\f0\b0 \
\
In [?]: for i, article in enumerate(set_articles):\
              if "1613 bis" in article.text:\
                  set_articles = set_articles[i:]\
                  break\
\
	
\f2\i\b 1.3. Cr\'e9er un DataFrame :  
\f3\b0 \

\f0\i0 \
In [?]: df = pd.DataFrame(data_scrapped, columns=["Art", "Date", "version", "Text", "Created"])\
\
	
\f1\b 1.4. Pour compter le nombre de renvois pr\'e9sents dans chaque article, il convient de modifier le texte de chaque article de telle mani\'e8re : 
\f0\b0 \
\
In [?]: def remplacer_articles(texte):\
    ...:     texte = texte.replace('articles', 'article')\
    ...: \
    ...:     # Remplacer "article X \'e0 Y" par une liste de num\'e9ros d'articles correspondante\
    ...:     texte = re.sub(r'article (\\d+) \'e0 (\\d+)', lambda match: ' '.join([f'article \{i\}' for i\
    ...:  in range(int(match.group(1)), int(match.group(2))+1)]), texte)\
    ...: \
    ...:     # Remplacer "article X, Y et Z" par une liste de num\'e9ros d'articles correspondante\
    ...:     texte = re.sub(r'article ((\\d+)(, |\\s+et\\s+))+(\\d+)', lambda match: ' '.join([f'artic\
    ...: le \{i\}' for i in sorted([int(x) for x in re.findall(r'\\d+', match.group(0))])]), texte)\
    ...: \
    ...:     # Remplacer "articles X et Y" par "article X article Y"\
    ...:     texte = re.sub(r'articles? (\\d+(?: et \\d+)+)', lambda match: ' '.join([f'article \{i\}'\
    ...:  for i in re.findall(r'\\d+', match.group(0))]), texte)\
    ...: \
    ...:     return texte\
    ...: \
    ...: \
\
In [?]: df['Text'] = df['Text'].apply(remplacer_articles) 
\f3\i #
\f2\b Ainsi, du texte sous la forme "les articles 4 \'e0 7" seront modifi\'e9es pour indiquer "article 4 article 5 article 6 article 7". 
\f0\i0\b0 \
\
\
	
\f1\b \ul Deuxi\'e8me \'e9tape - Premi\'e8re analyse : l'\'e9volution du nombre de renvois depuis 1980, au fil du temps \
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 \ulnone \
	
\f1\b 2.1 : fichiers contenant pour chaque ann\'e9e la version des articles du CGI en vigueur
\f0\b0 \
\
filename = "Datamodifi\'e9e.csv"\
\
# Boucler sur chaque ann\'e9e de 1979 \'e0 2023\
for year_limit in range(1979, 2024):\
\
    # Ouvrir le fichier CSV\
    with open(filename, encoding='utf-8') as file:\
        reader = csv.DictReader(file, delimiter=',')\
\
        # Cr\'e9ation d\'92un dictionnaire pour stocker les articles\
        articles = \{\}\
\
        # Cr\'e9ation d\'92un dictionnaire pour compter le nombre de renvois\
        num_articles = \{\}\
\
        # Boucler sur chaque ligne du fichier CSV\
        for row in reader:\
            # Extraction de l'ann\'e9e\
            date_obj = datetime.strptime(row['Date'], '%Y-%m-%d')\
            year = date_obj.year\
\
            # Si l'ann\'e9e est avant ou \'e9gale \'e0 la limite d\'e9finie\
            if year <= year_limit:\
                # Extraction du num\'e9ro de l'article et de sa version\
                article = row['Art']\
                version = int(row['version'])\
\
                # Si l'article n'est pas d\'e9j\'e0 dans le dictionnaire ou si la version est plus r\'e9cente\
                if article not in articles or version > articles[article][1]:\
                    # Ajouter l'article avec sa version\
                    articles[article] = (row, version)\
\
                    # Extraction du nombre de renvois pour cet article\
                    num_articles[article] = int(row['Num_articles'])\
                elif version == articles[article][1]:\
                    # Si la version est identique, utiliser le nombre de renvois pour cet article\
                    num_articles[article] = int(row['Num_articles'])\
\
    # Cr\'e9ation d\'92un nouveau fichier CSV avec les articles en vigueur pour l'ann\'e9e d\'e9termin\'e9e\
    with open('articles_\{\}.csv'.format(year_limit), mode='w', newline='', encoding='utf-8') as file:\
        writer = csv.writer(file, delimiter=',')\
\
        # \'c9criture de l'en-t\'eate\
        writer.writerow(['Date', 'Art', 'version', 'Num_articles'])\
\
        # Boucler sur chaque article enregistr\'e9 dans le dictionnaire\
        for article, (row, version) in articles.items():\
            # Formater la date en AAAA-MM-JJ\
            date_obj = datetime.strptime(row['Date'], '%Y-%m-%d')\
            date_str = date_obj.strftime('%Y-%m-%d')\
\
            # \'c9criture de la ligne correspondant \'e0 la version la plus r\'e9cente de l'article\
            writer.writerow([date_str, article, version, num_articles[article]])\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0
\cf2 \CocoaLigature0 	
\f1\b 2.2.\cf0 \CocoaLigature1  : somme des renvois pour chaque ann\'e9e et graphique
\f0\b0 \
\pard\pardeftab720\partightenfactor0
\cf0 \
# D\'e9limitation des ann\'e9es de d\'e9but et de fin\
annee_debut = 1979\
annee_fin = 2023\
\
# Initialisation du dictionnaire pour stocker les totaux par ann\'e9e\
annee_total = \{\}\
\
# Boucler \'e0 travers chaque ann\'e9e\
for annee in range(annee_debut, annee_fin+1):\
    # D\'e9finir le nom de fichier pour cette ann\'e9e\
    nom_fichier = f"articles_\{annee\}.csv"\
    # V\'e9rifier si le fichier existe\
    if os.path.isfile(nom_fichier):\
        # Charger le fichier CSV dans un DataFrame Pandas\
        df = pd.read_csv(nom_fichier)\
        # Calculer la somme de la colonne "Num_articles" (nombre de renvois)\
        total = df["Num_articles"].sum()\
        # Stocker le total dans le dictionnaire\
        annee_total[annee] = total\
\
# Conversion du dictionnaire en DataFrame Pandas\
df_total = pd.DataFrame.from_dict(annee_total, orient="index", columns=["Total"])\
# Tri des donn\'e9es par ann\'e9e croissante\
df_total = df_total.sort_index()\
\
# Cr\'e9ation d\'92un graphique avec M\cf2 at\cb3 plotlib (courbe rouge ; graphique 1)\
plt.plot(df_total.index, df_total["Total"]\expnd0\expndtw0\kerning0
, color='r')\kerning1\expnd0\expndtw0 \
plt.xlabel("Ann\'e9e")\
plt.ylabel("Nombre total de renvois")\
plt.title("Les renvois \'e0 d\'92autres articles au sein du CGI au fil du temps")\
\pard\pardeftab720\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
ax = plt.gca()\
ax.spines['top'].set_visible(False)\
ax.spines['right'].set_visible(False)\
ax.spines['bottom'].set_visible(True)\
ax.spines['left'].set_visible(True)\
ax.spines['bottom'].set_color('black')\
ax.spines['left'].set_color('black')\
ax.spines['bottom'].set_linewidth(2)\
ax.spines['left'].set_linewidth(2)\kerning1\expnd0\expndtw0 \
plt.show()\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \
#Graphique 2 (fonction polyn\cf2 omiale de degr\'e9 3): \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0

\f3\i \cf2 \CocoaLigature0 # Cr\'e9ation d\'92une figure et d\'92un axe
\f0\i0 \
fig, ax = plt.subplots()\
\

\f3\i # Ajout de la courbe avec une ligne plus \'e9paisse et une couleur rouge
\f0\i0 \
x = df_total.index\
y = df_total["Total"]\
f = np.polyfit(x, y, 3)\
p = np.poly1d(f)\
ax.plot(x, y, 'o', color="red", markersize=3)\
ax.plot(x, p(x), '-', color="blue", linewidth=2)\
\

\f3\i # Ajout des cadrillages
\f0\i0 \
ax.grid(
\f1\b True
\f0\b0 )\
\

\f3\i # Ajout des labels d'axes et du titre en gras
\f0\i0 \
ax.set_xlabel("Ann\'e9e")\
ax.set_ylabel("Nombre total de renvois")\
ax.set_title("Les renvois \'e0 d\'92autres articles au sein du CGI au fil du temps", fontweight='bold')\
\

\f3\i # Ajout d\'92une bordure autour de la courbe pour la rendre plus en relief
\f0\i0 \

\f1\b for
\f0\b0  spine 
\f1\b in
\f0\b0  ax.spines.values():\
    spine.set_edgecolor('gray')\
    spine.set_linewidth(1.5)\
  \

\f3\i # Affichage du graphique
\f0\i0 \
plt.show()\
\
	
\f1\b 2.3. : Autres analyses du dataset
\f0\b0 \
\
#Analyse des 10 articles contenant le plus de renvois (1980 & 2023) :\
\

\f3\i # Lecture du fichier CSV
\f0\i0 \
df = pd.read_csv('articles_2023.csv')\
\

\f3\i # Trier le dataframe par ordre d\'e9croissant de la colonne "Num_articles"
\f0\i0 \
df_sorted = df.sort_values(by=['Num_articles'], ascending=
\f1\b False
\f0\b0 )\
\

\f3\i # Extraire les 10 premiers articles avec le chiffre le plus \'e9lev\'e9
\f0\i0 \
top_articles = df_sorted.head(10)\
\

\f3\i # Cr\'e9er un graphique \'e0 partir des donn\'e9es extraites
\f0\i0 \
plt.figure(figsize=(8,6))\
plt.bar(top_articles['Art'], top_articles['Num_articles'])\
plt.title('Top 10 des articles du CGI contenant le plus de renvois en 1980', fontsize=14, fontweight='bold')\
plt.xlabel('Article', fontsize=12, fontweight='bold', color='black')\
plt.ylabel('Nombre de renvois', fontsize=12, fontweight='bold', color='black')\
plt.tick_params(axis='both', labelsize=12, width=2, color='black')\
plt.xticks(rotation=90)\
plt.grid(axis='y', linestyle='--', alpha=0.7)\
plt.gca().spines['bottom'].set_linewidth(2)\
plt.gca().spines['left'].set_linewidth(2)\
plt.show()\
\
\
#Analyse des 10 articles ayant subi le + de versions (en 2023) :\
\
# Lecture du fichier CSV\
df = pd.read_csv("articles_2023.csv")\
\
# Groupement des articles par nom et calcul du nombre de versions\
versions_count = df.groupby("Art")["version"].max().sort_values(ascending=False)[:10]\
\
# Cr\'e9ation d\'92un graphique en diagramme\
colors = ["#ff5733", "#33a7ff"] # Alternance entre rouge et bleu vif\
plt.bar(versions_count.index, versions_count.values, color=colors * 5)\
plt.title("Les 10 articles ayant subi le plus de versions")\
plt.xlabel("Nom de l'article")\
plt.ylabel("Nombre de versions")\cf4 \
plt.show()\cf0 \CocoaLigature1 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
	
\f1\b \ul Troisi\'e8me \'e9tape - Deuxi\'e8me analyse : Prestige de chaque article et Network Analysis 
\f0\b0 \ulnone \
\
In [?]: df['Art'] = df['Art'].str.lower()\
        df = df[df['version'] == 1] # On se concentre sur 2023\
\
In [?]:\
    ...: # Cr\'e9er un graphe vide\
         G = nx.Graph()\
    ...: # Ajouter chaque article comme un noeud du graphe\
    ...: for article in df['Art']:\
    ...:     G.add_node(article)\
    ...: \
    ...: # Parcourir chaque article et ajouter les ar\'eates correspondantes\
    ...: for index, row in df.iterrows():\
    ...:     article = row['Art']\
    ...:     text = row['Text']\
    ...:     for mentioned_article in df['Art']:\
    ...:         if mentioned_article != article and mentioned_article in text:\
    ...:             G.add_edge(article, mentioned_article)\
    ...: \
    ...: # Calculer la centralit\'e9 de degr\'e9 pour chaque noeud\
    ...: degree_centrality = nx.degree_centrality(G)\
    ...: \
    ...: # Trier les noeuds par ordre d\'e9croissant de centralit\'e9 de degr\'e9\
    ...: sorted_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], re\
    ...: verse=True)\
    ...: \
    ...: # Afficher les 10 articles les plus prestigieux\
    ...: for article, degree in sorted_nodes[:10]:\
    ...:     print(article, degree)\
    ...: \
article 16 0.14059590316573556\
article 15 0.09264432029795158\
article 8 0.08193668528864059\
article 6 0.06284916201117319\
article 1 a 0.053538175046554934\
article 14 0.053538175046554934\
article 10 0.051675977653631286\
article 5 0.05074487895716946\
article 7 0.05074487895716946\
article 39 0.05027932960893855\
\
	
\f1\b Le code ci-dessous permet de retrouver les occurrences du nom de l'article dans le texte des autres articles, et ensuite, de printer les 10 articles les plus prestigieux. 
\f0\b0 \
	\
	
\f1\b Pour d\'e9velopper encore plus l'analyse, une Network analysis est effectu\'e9e : 
\f0\b0 \
\
In [?]:  import networkx as nx\
    ...: import matplotlib.pyplot as plt\
    ...: \
    ...: # Cr\'e9er le graphique vide\
    ...: G = nx.DiGraph()\
    ...: \
    ...: # Ajouter les noeuds correspondants aux articles\
    ...: for article in df['Art']:\
    ...:     G.add_node(article)\
    ...: \
    ...: # Ajouter les liens entre les articles\
    ...: for i, row in df.iterrows():\
    ...:     text = row['Text']\
    ...:     art = row['Art']\
    ...:     # Chercher les articles mentionn\'e9s dans le texte\
    ...:     mentions = [s for s in df['Art'] if s != art and s in text.lower()]\
    ...:     # Ajouter les liens\
    ...:     for mention in mentions:\
    ...:         G.add_edge(art, mention)\
    ...: \
    ...: # Dessiner le graphe\
    ...: plt.figure(figsize=(40,40))\
    ...: pos = nx.random_layout(G, seed=42)\
    ...: nx.draw_networkx(G, pos, node_size=3, node_color='lightblue', edge_color='#1f78b4', arrowsize=4,\
    ...:                  font_size=2.5, font_weight='bold', with_labels=True, width = 0.05)\
    ...: \
    ...: # Afficher le graphe avec un fond blanc\
    ...: ax = plt.gca()\
    ...: ax.set_facecolor('white')\
    ...: \
    ...: plt.axis('off')\
    ...: plt.savefig('graphN.png', dpi=300, bbox_inches ='tight')	\
\
\
	
\f1\b Le graph est suffisamment grand pour que, malgr\'e9 le nombre tr\'e8s important de donn\'e9es l'ensemble soit lisible (fichier de +60Mo).
\f0\b0  \
\
}