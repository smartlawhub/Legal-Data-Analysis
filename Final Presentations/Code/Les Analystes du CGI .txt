
	Première étape - SCRAPPING : scrapper le CGI 

In [?]: for code in CGI:
   ...:     driver.get("https://www.legifrance.gouv.fr/" + codes[code])
   ...:     soup = BeautifulSoup(driver.page_source)
   ...:     set_articles = soup.find_all("a", string=re.compile("^Article"))
   ...:     for article in set_articles:
   ...:         legiarti_match = re.search("LEGIARTI.*?#", article.get("href"))
   ...:         if legiarti_match:
   ...:             legiarti = legiarti_match.group()[:-1]
   ...:         else:
   ...:             continue
   ...:         driver.get("https://www.legifrance.gouv.fr/codes/article_lc/" + legiarti)
   ...:         time.sleep(1)
   ...:         driver.find_element(By.XPATH, ".//button[@data-articleid='" + legiarti + "']|.//bu
   ...: tton[@data-articlecid='" + legiarti + "']").click()
   ...:         time.sleep(2)
   ...:         article_num = driver.find_element(By.CLASS_NAME, "name-article").text
   ...:         version_urls = [x.get_attribute("href") for x in driver.find_elements(By.CLASS_NAM
   ...: E, "version")]
   ...:         count_version = len(version_urls)
   ...:         for version_url in [x for x in version_urls if x is not None]:
   ...:             driver.get(version_url)
   ...:             legiver = re.search("LEGIARTI.*?/", version_url).group()[:-1]
   ...:             time.sleep(1)
   ...:             driver.find_element(By.XPATH, ".//button[@data-articleid='" + legiver + "']|//
   ...: button[@data-num='" + article_num.split(" ")[-1] + "']").click()
   ...:             time.sleep(1)
   ...:             try:
   ...:                 origin = driver.find_element(By.CLASS_NAME, "date").text
   ...:             except:
   ...:                 origin = ""
   ...:             article_text = driver.find_elements(By.CLASS_NAME, "content")[2].text
   ...:             temp_list = [article_num, version_url.split("/")[-2], count_version, article_t
   ...: ext, origin]
   ...:             count_version -= 1
   ...:             data_scrapped.append(temp_list)
   ...: driver.close()
   ...: 


	1.2. En cas d'erreur lors du scrapping, une fonction permet de modifier la variable set_articles pour faire reprendre le scrapping à un endroit voulu. Exemple : 

In [?]: for i, article in enumerate(set_articles):
              if "1613 bis" in article.text:
                  set_articles = set_articles[i:]
                  break

	1.3. Créer un DataFrame :  

In [?]: df = pd.DataFrame(data_scrapped, columns=["Art", "Date", "version", "Text", "Created"])

	1.4. Pour compter le nombre de renvois présents dans chaque article, il convient de modifier le texte de chaque article de telle manière : 

In [?]: def remplacer_articles(texte):
    ...:     texte = texte.replace('articles', 'article')
    ...: 
    ...:     # Remplacer "article X à Y" par une liste de numéros d'articles correspondante
    ...:     texte = re.sub(r'article (\d+) à (\d+)', lambda match: ' '.join([f'article {i}' for i
    ...:  in range(int(match.group(1)), int(match.group(2))+1)]), texte)
    ...: 
    ...:     # Remplacer "article X, Y et Z" par une liste de numéros d'articles correspondante
    ...:     texte = re.sub(r'article ((\d+)(, |\s+et\s+))+(\d+)', lambda match: ' '.join([f'artic
    ...: le {i}' for i in sorted([int(x) for x in re.findall(r'\d+', match.group(0))])]), texte)
    ...: 
    ...:     # Remplacer "articles X et Y" par "article X article Y"
    ...:     texte = re.sub(r'articles? (\d+(?: et \d+)+)', lambda match: ' '.join([f'article {i}'
    ...:  for i in re.findall(r'\d+', match.group(0))]), texte)
    ...: 
    ...:     return texte
    ...: 
    ...: 

In [?]: df['Text'] = df['Text'].apply(remplacer_articles)

	Ainsi, du texte sous la forme "les articles 4 à 7" seront modifiées pour indiquer "article 4 article 5 article 6 article 7". 




	Deuxième étape - Première analyse : l'évolution du nombre de renvois depuis 1980

	1.1. Il convient de compter à présent le nombre de renvois dans chaque article : 

In [?] : df['Num_articles'] = df['Text'].apply(lambda text: text.count('article'))

	Ainsi une nouvelle colonne est créée, et permettra ensuite sous Excel de faire un graphique sur l'évolution du nombre de renvois. 




	Troisième étape - Deuxième analyse : Prestige de chaque article et Network Analysis 

In [?]: df['Art'] = df['Art'].str.lower()
        df = df[df['version'] == 1] # On se concentre sur 2023

In [?]:
    ...: # Créer un graphe vide
         G = nx.Graph()
    ...: # Ajouter chaque article comme un noeud du graphe
    ...: for article in df['Art']:
    ...:     G.add_node(article)
    ...: 
    ...: # Parcourir chaque article et ajouter les arêtes correspondantes
    ...: for index, row in df.iterrows():
    ...:     article = row['Art']
    ...:     text = row['Text']
    ...:     for mentioned_article in df['Art']:
    ...:         if mentioned_article != article and mentioned_article in text:
    ...:             G.add_edge(article, mentioned_article)
    ...: 
    ...: # Calculer la centralité de degré pour chaque noeud
    ...: degree_centrality = nx.degree_centrality(G)
    ...: 
    ...: # Trier les noeuds par ordre décroissant de centralité de degré
    ...: sorted_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], re
    ...: verse=True)
    ...: 
    ...: # Afficher les 10 articles les plus prestigieux
    ...: for article, degree in sorted_nodes[:10]:
    ...:     print(article, degree)
    ...: 
article 16 0.14059590316573556
article 15 0.09264432029795158
article 8 0.08193668528864059
article 6 0.06284916201117319
article 1 a 0.053538175046554934
article 14 0.053538175046554934
article 10 0.051675977653631286
article 5 0.05074487895716946
article 7 0.05074487895716946
article 39 0.05027932960893855

	Le code ci-dessous permet de retrouver les occurrences du nom de l'article dans le texte des autres articles, et ensuite, de printer les 10 articles les plus prestigieux. 
	
	Pour développer encore plus l'analyse, une Network analysis est effectuée : 

In [?]:  import networkx as nx
    ...: import matplotlib.pyplot as plt
    ...: 
    ...: # Créer le graphique vide
    ...: G = nx.DiGraph()
    ...: 
    ...: # Ajouter les noeuds correspondants aux articles
    ...: for article in df['Art']:
    ...:     G.add_node(article)
    ...: 
    ...: # Ajouter les liens entre les articles
    ...: for i, row in df.iterrows():
    ...:     text = row['Text']
    ...:     art = row['Art']
    ...:     # Chercher les articles mentionnés dans le texte
    ...:     mentions = [s for s in df['Art'] if s != art and s in text.lower()]
    ...:     # Ajouter les liens
    ...:     for mention in mentions:
    ...:         G.add_edge(art, mention)
    ...: 
    ...: # Dessiner le graphe
    ...: plt.figure(figsize=(40,40))
    ...: pos = nx.random_layout(G, seed=42)
    ...: nx.draw_networkx(G, pos, node_size=3, node_color='lightblue', edge_color='#1f78b4', arrowsize=4,
    ...:                  font_size=2.5, font_weight='bold', with_labels=True, width = 0.05)
    ...: 
    ...: # Définir une résolution DPI de 300
    ...: 
    ...: 
    ...: # Afficher le graphe avec un fond blanc
    ...: ax = plt.gca()
    ...: ax.set_facecolor('white')
    ...: 
    ...: plt.axis('off')
    ...: plt.savefig('graphN.png', dpi=300, bbox_inches ='tight')	


	Le graph est suffisamment grand pour que, malgré le nombre très important de données l'ensemble soit lisible (fichier de +60Mo). 

